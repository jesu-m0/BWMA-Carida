% -----------------------------------------------
% Template for SMC 2023
% based on SMC 2022 template
% -----------------------------------------------

\documentclass{article}
\usepackage{smc}
\usepackage{times}
\usepackage{ifpdf}
\usepackage[english]{babel}
\usepackage{cite}
\usepackage{listings}

%%%%%%%%%%%%%%%%%%%%%%%% Some useful packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%% See related documentation %%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{amsmath} % popular packages from Am. Math. Soc. Please use the 
%\usepackage{amssymb} % related math environments (split, subequation, cases,
%\usepackage{amsfonts}% multline, etc.)
%\usepackage{bm}      % Bold Math package, defines the command \bf{}
%\usepackage{paralist}% extended list environments
%%subfig.sty is the modern replacement for subfigure.sty. However, subfig.sty 
%%requires and automatically loads caption.sty which overrides class handling 
%%of captions. To prevent this problem, preload caption.sty with caption=false 
%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}


%user defined variables
\def\papertitle{Angular: Platform and Framework for Web Application Development}
\def\firstauthor{Jesús Moreno Durán}
\def\secondauthor{Second author}
\def\thirdauthor{Third author}

% adds the automatic
% Saves a lot of output space in PDF... after conversion with the distiller
% Delete if you cannot get PS fonts working on your system.

% pdf-tex settings: detect automatically if run by latex or pdflatex
\newif\ifpdf
\ifx\pdfoutput\relax
\else
   \ifcase\pdfoutput
      \pdffalse
   \else
      \pdftrue
\fi

\ifpdf % compiling with pdflatex
  \usepackage[pdftex,
    pdfauthor={\firstauthor, \secondauthor, \thirdauthor},
    bookmarksnumbered, % use section numbers with bookmarks
    pdfstartview=XYZ % start with zoom=100% instead of full screen; 
                     % especially useful if working with a big screen :-)
   ]{hyperref}
  %\pdfcompresslevel=9

  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are and their extensions so 
  %you won't have to specify these with every instance of \includegraphics
  \graphicspath{{./figures/}}
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}

  \usepackage[figure,table]{hypcap}

\else % compiling with latex
  \usepackage[dvips,
    bookmarksnumbered, % use section numbers with bookmarks
    pdfstartview=XYZ % start with zoom=100% instead of full screen
  ]{hyperref}  % hyperrefs are active in the pdf file after conversion

  \usepackage[dvips]{epsfig,graphicx}
  % declare the path(s) where your graphic files are and their extensions so 
  %you won't have to specify these with every instance of \includegraphics
  \graphicspath{{./figures/}}
  \DeclareGraphicsExtensions{.eps}

  \usepackage[figure,table]{hypcap}
\fi

%setup the hyperref package - make the links black without a surrounding frame
\hypersetup{
    colorlinks,%
    citecolor=black,%
    filecolor=black,%
    linkcolor=black,%
    urlcolor=black
}


% Title.
% ------
\title{\papertitle}

% Authors
% Please note that submissions are NOT anonymous, therefore 
% authors' names have to be VISIBLE in your manuscript. 
%
% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
%   {\firstauthor} {Affiliation1 \\ %
%     {\tt \href{mailto:author1@smcnetwork.org}{author1@smcnetwork.org}}}

%Two addresses
%--------------
% \twoauthors
%   {\firstauthor} {Affiliation1 \\ %
%     {\tt \href{mailto:author1@smcnetwork.org}{author1@smcnetwork.org}}}
%   {\secondauthor} {Affiliation2 \\ %
%     {\tt \href{mailto:author2@smcnetwork.org}{author2@smcnetwork.org}}}

% Three addresses
% --------------
 \oneauthor
   {\firstauthor} {Fulda University of Applied Science \\ %
     {\tt \href{mailto:jesus.moreno-duran@informatik.hs-fulda.de}{jesus.moreno-duran@informatik.hs-fulda.de}}}


% ***************************************** the document starts here ***************
\begin{document}
%
\capstartfalse
\maketitle
\capstarttrue

\tableofcontents

%
\begin{abstract}
    In this write-up, we will be discussing Angular, a framework and platform that enables the development of client-side single-page applications (SPA) through the use of TypeScript and HTML. Additionally, we will provide a hands-on demonstration of how to create a web application using Angular, specifically one that allows for car sharing and ride-sharing. The platform enables users to publish travel routes and fares, while others can join in. However, it's important to mention that the application's implementation is based on dummy data and can only be accessed through the frontend.
\end{abstract}
%

\section{Introduction}\label{sec:introduction}

    Sharing cars has gained immense popularity in recent times due to its cost-effective and travel-efficient benefits. However, managing coordination between users can often be challenging. To tackle this issue, an Angular-based web application has been created that enables users to share rides and travel together seamlessly, thereby reducing traffic congestion and also promoting environmental sustainability by reducing the number of cars on the road.

\subsection{What is Angular?}
    Angular is a powerful platform and open-source framework used for developing single-page applications (SPA). It was created and is maintained by Google, and provides a complete solution for building dynamic web applications using HTML, CSS, and TypeScript.

    The framework is designed to simplify the development process by providing a structured approach for building complex client-side applications. Angular utilizes declarative templates and dependency injection to separate application logic from presentation, allowing developers to focus on the core business logic of their applications.

    One of the key features of Angular is its ability to handle complex data models and provide a scalable architecture for building large applications. It also includes a robust set of built-in tools and libraries that simplify common tasks such as routing, forms handling, and testing.

    Angular's popularity is due in part to its flexibility and ability to integrate with other tools and technologies, making it an ideal choice for a wide range of web development projects. 

    Angular was initially developed as a private project by Google, with the first version being released internally in 2009. However, it wasn't until September 2016 that the framework was officially made available to the public, with the release of Angular 2. This move to open-source made Angular accessible to a wider community of developers, leading to its rapid growth and adoption as a leading framework for building single-page applications. Despite its transition to an open-source project, Angular continues to be supported and maintained by Google, ensuring that it remains a cutting-edge technology for years to come.

\subsection{Features of Angular}
    
\begin{itemize}

    \item TypeScript: Angular is built using TypeScript, a statically typed superset of JavaScript that provides improved error checking and code maintainability.
    \item Components: Angular provides a powerful component based architecture that allows develop complex applications by composing simple components. This makes it easy to reuse and share code, and simplifies the development of large-scale applications.
    \item Dependency Injection: Angular uses dependency injection to manage object creation and simplify the development of complex applications.
    \item Declarative Templates: Angular utilizes declarative templates to separate application logic from presentation, making it easier to maintain and scale applications.
    \item Routing: Angular includes a powerful routing system that enables developers to build applications with multiple views and navigate between them.
    \item Progressive Web Apps (PWA): Angular provides support for building Progressive Web Apps, which can be installed on users' devices and function like native apps. PWAs built with Angular are fast, responsive, and can work offline, making them a powerful tool for engaging users and improving user experience.


  \end{itemize}
  

  \subsection{How to create a project}
  In order to create a project in Angular you need to follow these steps:
  \begin{enumerate}
    \item Install a code editor. To edit the project, you need to have a code editor installed on your system. Visual Studio Code is a good option as it has extensions to facilitate development in Angular.
  \item Install Node.js and npm. Angular uses Node.js and npm as package managers. Hence, you need to have them installed on your system.
  \item Install the Angular client. To install the Angular client, use the command line tool and run this command: \verb|npm install -g @angular/cli|. It will install the Angular client globally on your system.
  \item Create a new project. To create a new project, execute: \verb|npm new [project-name]|. This command creates a new folder with the name of your project and generates the basic structure of the project in it.
  \item Run the project. Navigate to the project folder using the command \verb|cd [project-name]| and then execute the following command: \verb|ng serve -o|. This command compiles the project and starts it in your default browser, allowing you to see the changes in real-time as you make them.
  \end{enumerate}


  \section{State of the Art}
  \label{sec:state_of_the_art}

\subsection{Angular vs React}

Angular and React are both popular JavaScript frameworks used for building web applications. Differences:

\begin{itemize}
  \item Flexibility: React is known for its flexibility, giving developers more freedom to structure their projects and choose their preferred syntax. It also allows the use of different complementary technologies. Angular, on the other hand, offers a stricter project structure and syntax that can be beneficial for novice developers or large teams needing to follow a defined standard.
  \item Origin: Google develops and maintains Angular, while Facebook is responsible for React.
  \item Typing: Angular uses Typescript, a typed programming language that provides greater security and ease of maintenance for large projects. React, on the other hand, only requires JavaScript and JSX for component creation.
  \item Templates: Angular uses HTML templates for component creation, which can be beneficial for web designers and make it easier to create reusable components. In contrast, React uses JSX, a JavaScript extension that allows for component creation with a syntax similar to HTML.
  \item Data binding: Angular allows for bidirectional data binding, where changes to the data model are automatically reflected in the view. React, however, uses unidirectional data binding, requiring changes in the data model to be explicitly passed to the view using props.
\end{itemize}


\subsection{Angular information sources}
Learn Angular is not difficult at all. There are several sources where you can learn. 

YouTube is a great resource for finding channels that offer tutorials for beginners. For example, 'Amigos Code' provides straightforward tutorials that are easy to follow. 'FreeCodeCamp.org' and 'Traversy Media' are also excellent channels to check out. Additionally, using the search bar can help you find exactly what you need to learn.

Always is a good option take courses in platforms like Udemy, Coursera or others similar.

There are several resources available to developers looking for help with Angular. The official Angular documentation is a recommended source, as it provides comprehensive information about the framework. In addition, there are several forums such as 'Stack Overflow', 'Reddit' and 'Angular Discussions', where developers can ask specific questions and get answers from the community. These resources can be useful for troubleshooting, gathering information and learning best practices related to Angular development.

\section{Methodology}  \label{sec:methodology}







  \subsection{How to create a project}
This triggered that NLP has been traditionally a complex problem to solve\cite{jones1992natural}. However, Recurrent Neural Networks and two significant advances - one in 2017 and another in 2019 - brought substantial improvements to NLP. In 2017, a new form of deep learning model called Transformer\cite{wang2019r} made it possible to parallelize ML training more efficiently, resulting in vastly improved accuracies.
In 2019, Google introduced Bidirectional Encoder Representations from Transformers (BERT)\cite{kamath2022bidirectional}, which improves the above Transformer architecture. Straightaway BERT helped achieve state-of-the-art performance\cite{DBLP:journals/corr/abs-1810-04805} on several NLP tasks such as reading comprehension, text extraction, sentiment analysis, etc. These two advancements meant that NLP could easily outdo average humans in many tasks and in some cases, even exceed the performance of subject matter experts. 
\subsection{Our question}
But does recurrent neural network propagation of information works similar as our brain does? We propose to look at brain activity of subjects reading naturalistic text as a source of additional information for interpreting neural networks. And to do this we use brain imaging recordings of subjects reading complex natural text to interpret word and sequence embeddings from 4 recent NLP models - ELMo, USE, BERT and Transformer-XL, and study how their representations differ across layer depth, context length, and attention type.

\section{Bla}
\label{sec:bla}

Most work investigating language in the brain has been done in a controlled experiment setup where two conditions are contrasted\cite{friederici2011brain}. These conditions typically vary in complexity (simple vs. complex sentences), in the presence or absence of a linguistic property (sentences vs. lists of words) or in the presence or absence of incongruities (e.g. semantic surprisal)\cite{friederici2011brain}. A few researchers instead use naturalistic stimulus such as stories\cite{brennan2012syntactic}. Some use predictive models of brain activity as a function of multi-dimensional features spaces describing the different properties of the stimulus\cite{wehbe2014simultaneously}.

A few previous works have used neural network representations as a source of feature spaces to model brain activity. Wehbe et al.\cite{wehbe2014simultaneously} aligned the MEG brain activity we use here with a Recurrent Neural Network (RNN), trained on an online archive of Harry Potter Fan Fiction. The authors aligned brain activity with the context vector and the word embedding, allowing them to trace sentence comprehension at a word-by-word level. Jain and Huth\cite{jain2018incorporating} aligned layers from a Long Short-Term Memory (LSTM) model to fMRI recordings of subjects listening to stories to differentiate between the amount of context maintained by each brain region. Other approaches rely on computing surprisal or cognitive load metrics using neural networks to identify processing effort in the brain, instead of aligning entire representations\cite{frank2015erp}.

There is also a little work that evaluates or improves NLP models through brain recordings. Søgaard\cite{sogaard2016evaluating} proposes to evaluate whether a word embedding contains cognition-relevant semantics by measuring how well they predict eye tracking data and fMRI recordings. Fyshe et al.\cite{fyshe2014interpretable} build a non-negative sparse embedding for individual words by constraining the embedding to also predict brain activity well and show that the new embeddings better align with behavioral measures of semantics.


\section{adsfasdf}\label{sec:sdafdsda}

We investigate how the representations of all four networks (ELMO, BERT, USE and T-XL) change as we provide varying lengths of context. We compute the representations x in each available intermediate layer ([1, 2] for ELMo; [1, 12] for BERT; the layer is the output embedding for USE; and [1, 19] for T-XL). We compute x for word w by passing the most recent k words through the network.

\subsection{fMRI and MEG data} 
We use fMRI and MEG data which complement each other very well. fMRI is sensitive to the change in oxygen level in the blood which is a consequence to neural activity, it has high spatial resolution (2-3 mm) and low temporal resolution (multiple seconds). MEG measures the change in the magnetic field outside the skull due to neural activity, it has low spatial resolution (multiple cm) and high temporal resolution (up to 1KHz). We use fMRI data published by Wehbe et al.\cite{wehbe2014simultaneously}: 8 subjects read chapter 9 of Harry Potter and the Sorcerer’s stone\cite{rowling2012harry} which was presented one word at a time for a fixed duration of 0.5 seconds each, and 45 minutes of data were recorded. The fMRI sampling rate (TR) was 2 seconds. The same chapter was shown by Wehbe et al.\cite{wehbe2014aligning} to 3 subjects in MEG with the same rate of 0.5 seconds per word. Details about the data and preprocessing can be found in the supplementary materials.

\subsection{Encoding models} 
For each type of network-derived representation x, we estimate an encoding model that takes x as input and predicts the brain recording associated with reading the same k words that were used to derive x. We estimate a function f, such that f(x) = y, where y is the brain activity recorded with either MEG or fMRI. We follow previous work\cite{wehbe2014simultaneously} and model f as a linear function, regularized by the ridge penalty. The model is trained via four-fold cross-validation and the regularization parameter is chosen via nested cross-validation.

\subsection{Evaluation of predictions}
We evaluate the predictions from each encoding model by using them in
a classification task on held-out data, in the four-fold cross-validation setting. The classification task is to predict which of two sets of words was being read based on the respective feature representations of these words\cite{wehbe2014simultaneously,wehbe2014aligning}. This task is performed between sets of 20 consecutive TRs in fMRI (accounting for the slowness of the hemodynamic response), and sets of 20 randomly sampled words in MEG. The classification is repeated a large number of times and an average classification accuracy is obtained for each voxel in fMRI and for each sensor/timepoint in MEG. We refer to this accuracy of matching the predictions of an encoding model to the correct brain recordings as "prediction accuracy". The final fMRI results are reported on the MNI template, and we use pycortex to visualize them\cite{gao2015pycortex}. 

\begin{figure}[t]
\centering
\includegraphics[width=1.1\columnwidth]{pycortex.png}
\caption{Amount of group 1b regions and group 2 regions predicted well by each network-derived representation: a 10-word representation corresponding to the 10 most recent words shown to the participant (Red) and a word-embedding corresponding to the last word (Blue). White indicates that both representations predict the specified amount of the regions well (about 0.7 threshold).\label{fig:pycortex}}
\end{figure}

\subsection{Proof of concept}
Since MEG signals are faster than the rate of word presentation, they are more
appropriate to study the components of word embeddings than the slow fMRI signals that cannot be
attributed to individual words. We know that a word embedding learned from a text corpus is likely to
contain information related to the number of letters and part of speech of a word. We show in section
4 of the supplementary materials that the number of letters of a word and its ELMo embedding
predict a shared portion of brain activity early on (starting 100ms after word onset) in the back of the
MEG helmet, over the visual cortex. Indeed, this region and latency are when we expect the visual
information related to a word to be processed (Sudre et al., 2012). Further, a word’s part of speech
and its ELMo embedding predict a shared portion of brain activity around 200ms after word onset in
the left front of the MEG sensor. Indeed, we know from electrophysiology studies that part of speech
violations incur a response around 200ms after word onset in the frontal lobe (Frank et al., 2015). We
conclude from these experiments that the ELMo embedding contains information about the number
of letters and the part of speech of a word. Since we knew this from the onset, this experiment serves
as a proof of concept for using our approach to interpret information in network representations.


\section{Headings}
First level headings are in Times 12~pt bold, centered with 1 line of space above the section head, and 1/2 space below it.
For a section header immediately followed by a subsection header, the space should be merged.

\subsection{Second Level Headings}
Second level headings are in Times 10~pt bold, flush left, with 1 line of space above the section head, and 1/2 space below it. The first letter of each significant word is capitalized.

\subsubsection{Third Level Headings}
Third level headings are in Times 10~pt italic, flush left, with 1/2 line of space above the section head, and 1/2 space below it. The first letter of significant words is capitalized.

Using more than three levels of headings is strongly discouraged.





\section{Floats and equations}

\subsection{Equations}
Equations should be placed on separated lines and numbered. The number should be on the right side, in parentheses.
\begin{equation}
r=\sqrt[13]{3}
\label{eq:BP}
\end{equation}
Always refer to equations like this: ``Equation (\ref{eq:BP}) is of particular interest because...''

\subsection{Figures, Tables and Captions}
\begin{table}[t]
 \begin{center}
 \begin{tabular}{|l|l|}
  \hline
  String value & Numeric value \\
  \hline
  Hej SMC! & 2023 \\
  \hline
 \end{tabular}
\end{center}
 \caption{Table captions should be placed below the table,  like this.}
 \label{tab:example}
\end{table}

All artwork must be centered, neat, clean and legible. Figures should be centered, neat, clean
and completely legible. All lines should be thick and dark enough for purposes of reproduction. Artwork should not be hand-drawn. The proceedings will be distributed in electronic form only, therefore color figures are allowed. However, you may want to check that your figures are understandable even if they are printed in black-and-white.


Numbers and captions of figures and tables always appear below the figure/table.
Leave 1 line space between the figure or table and the caption.
Figure and tables are numbered consecutively. 
Captions should be Times 10pt. Place tables/figures in the text as close to the reference as possible, 
and preferably at the top of the page.

Always refer to tables and figures in the main text, for example: ``see Fig. \ref{fig:example} and \tabref{tab:example}''.
Figures and tables may extend across both columns to a maximum width of 17.2cm.

Vectorial figures are preferred, e.g., eps. When using \texttt{Matlab}, export using either (encapsulated) Postscript or PDF format. In order to optimize readability, the font size of text within a figure should be no smaller than
that of footnotes (8~pt font-size). If you use bitmap figures, make sure that the resolution is high enough for print quality. 

\begin{figure}[t]
\centering
\includegraphics[width=0.6\columnwidth]{figure}
\caption{Figure captions should be placed below the figure, 
exactly like this.\label{fig:example}}
\end{figure}


\subsection{Footnotes}
You can indicate footnotes with a number in the text \footnote{This is a footnote example.},
but try to work the content into the main text.Use 8~pt font-size for footnotes.  Place the footnotes at the bottom of the page 
on which they appear. Precede the footnote with a 0.5~pt horizontal rule.

\section{Citations}
All bibliographical references should be listed at the end, inside a section named ``REFERENCES''. References must be numbered in order of appearance. You should avoid listing references that do not appear in the text.

Reference numbers in the text should appear within square brackets, such as in~\cite{Someone:00} or~\cite{Someone:00,Someone:04,Someone:09}. The reference format is the standard IEEE one. We highly recommend you use BibTeX 
to generate the reference list.

\section{Conclusions}
Please, submit full-length papers. Submission is fully electronic and automated through the Conference Web Submission System. \underline{Do not} send papers directly by e-mail.


\begin{acknowledgments}
At the end of the Conclusions, acknowledgements to people, projects, funding agencies, etc. can be included after the second-level heading  ``Acknowledgments'' (with no numbering).
\end{acknowledgments} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%bibliography here
\bibliography{smc2023bib}

\end{document}
